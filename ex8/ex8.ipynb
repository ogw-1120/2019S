{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題8 線形回帰\n",
    "\n",
    "配点\n",
    "- Q1.1 1P\n",
    "- Q1.2 1P\n",
    "- Q1.3 2P\n",
    "- Q1.4 1P\n",
    "- Q2 2P\n",
    "- Q3 3P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュールのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 特徴間の相関係数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irisデータセットの特徴量`petal_length`と`petal_width`の関係を散布図で可視化すると以下のように2つの特徴量間に関係があることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.scatter(iris['petal_length'], iris['petal_width'], c='blue');\n",
    "\n",
    "### scikit-learnからデータロードする場合\n",
    "# from sklearn.datasets import load_iris\n",
    "# iris = load_iris()\n",
    "#plt.figure(figsize=(7,5))\n",
    "#plt.xlabel(iris[\"feature_names\"][2])\n",
    "#plt.ylabel(iris[\"feature_names\"][3])\n",
    "#plt.scatter(iris['data'][:,2],iris['data'][:,3],c='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irisデータセットの特徴量`petal_length`と`petal_width`のデータを`NumPy`の1次元配列として受け取って、それらの相関係数を返す以下の関数`petal_corr()`を完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def petal_corr(length, width):\n",
    "    return ### 1次元配列lengthと1次元配列widthの相関係数を計算するコード ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数`petal_corr()`が完成したら以下のセルを実行して動作を確認してください。以下では`pandas`シリーズオブジェクトの`values`属性を用いて特徴量データを`NumPy`の配列にして関数`petal_corr()`の引数に渡しています。相関係数は$\\simeq0.96$となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_corr(iris['petal_length'].values, iris['petal_width'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 特徴量の標準化\n",
    "\n",
    "以下では、線形回帰により特徴量`petal_length`から特徴量`petal_width`を予測するような仮説関数のパラメータを学習することを考えます。\n",
    "\n",
    "まず準備として、特徴量`petal_length`を入力$X$、特徴量`petal_length`を出力$y$としてそれぞれを標準化します。\n",
    "\n",
    "`NumPy`の任意の行数、列数の2次元配列を受け取り、各要素の値をその列の平均値と標準偏差を用いて標準化した配列を返す`normalizer()`関数を完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(mat):\n",
    "    return ### 配列matを列ごとに標準化した配列を計算するコード ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数`normalizer()`が完成したら以下のセルを実行して動作を確認してください。標準化された入力$X_{norm}$、標準化された出力$y_{norm}$の要素の和はそれぞれ$\\simeq 4.9e^{-14} $, $\\simeq-4.2e^{-14}$となります。（評価はコードを見て行うので必ずしも一致していなくとも構いません）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "X=iris[['petal_length']].values # 入力X\n",
    "y=iris[['petal_width']].values # 出力y\n",
    "\n",
    "### scikit-learnからデータロードした場合 \n",
    "# X=(iris['data'][:,2])[:,np.newaxis] # 列ベクトルにする\n",
    "# y=(iris['data'][:,3])[:,np.newaxis] # 列ベクトルにする\n",
    "\n",
    "X_norm = normalizer(X) # 入力の標準化\n",
    "y_norm = normalizer(y) # 出力の標準化\n",
    "\n",
    "print(X_norm.sum())\n",
    "print(y_norm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、入力の各データにバイアス項($x_0=1$)を追加するため、入力の先頭列に1を要素とする列ベクトルを挿入します。これにより、入力,　出力はデータ数を$m$として以下のような行列（$m \\times 2$) とベクトル($m \\times 1$)になります。\n",
    "\n",
    "$\n",
    "  X = \\left(\n",
    "    \\begin{array}{cc}\n",
    "      1 &   x^{(1)} \\\\\n",
    "      1 &   x^{(2)}  \\\\\n",
    "      ... & ... \\\\\n",
    "      1 &  x^{(m)}\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$\n",
    "\n",
    "$x^{(i)}$は標準化された特徴量petal_length。\n",
    "\n",
    "$\n",
    "  y = \\left(\n",
    "    \\begin{array}{c}\n",
    "       y^{(1)}  \\\\\n",
    "      ...\\\\\n",
    "        y^{(m)}  \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$\n",
    "\n",
    "$y^{(i)}$は標準化された特徴量petal_width。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm=np.insert(X_norm, 0, np.ones((1,X.shape[0]), dtype=int), axis=1) # バイアス項の追加\n",
    "\n",
    "print(X_norm[:10,:])\n",
    "print(y_norm[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 最急降下法\n",
    "\n",
    "以下では、最急降下法により、訓練データセットを元に入力から出力を予測する仮説関数のパラメータを学習する`graddes`関数を実装します。\n",
    "\n",
    "`graddes`関数では第1引数に入力のデータ行列（データ数($m$)$\\times$(特徴量数+バイアス)($n$))、第2引数に入力の各データに対する出力（正解）のベクトル（$m\\times1$）、第3引数に学習率、第4引数に学習の繰り返し（各繰り返しをエポックと呼ぶ）の回数を受け取ります。\n",
    "\n",
    "これらの引数を元に、`graddes`関数ではパラメータの学習を行い、以下を返します\n",
    "- エポックごとのコスト関数の値を要素とするリスト\n",
    "- 最終的なパラメータの値を要素とする配列（$n\\times1$）を返します。\n",
    "\n",
    "\n",
    "\n",
    "パラメータを$\\theta=(\\theta_0, \\theta_1, ..., \\theta_{n-1})^T$\n",
    "\n",
    "仮説関数を$h(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+....+\\theta_{n-1}x_{n-1}$\n",
    "\n",
    "入力を$\n",
    "  X = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      x_0^{(1)} &   x_1^{(1)} & ... &   x_{n-1}^{(1)}  \\\\\n",
    "      ... & ...& ...&...\\\\\n",
    "      x_0^{(m)} &  x_1^{(m)} & ... &   x_{n-1}^{(m)}  \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$ \n",
    "\n",
    "$X$において$x_0^{(i)}=1$\n",
    "\n",
    "出力を$y=(y^{(1)}, y^{(2)}, ..., y^{(m)})^T$\n",
    "\n",
    "とすると、最急降下法ではコスト関数を\n",
    "\n",
    "$J(\\theta)=\\frac{1}{2m}\\Sigma_{i=1}^m (h(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "として、入力$X$の各特徴量$x_j$に対するパラメータ$\\theta_j$を以下の様に更新していきます。\n",
    "\n",
    "$\\theta_j:= \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} =  \\theta_j - \\frac{\\alpha}{m} \\Sigma_{i=1}^m ((h(x^{(i)})-y^{(i)})x^{(i)}_j)$\n",
    "\n",
    "パラメータ全体を以下のように一度に更新することもできます。\n",
    "\n",
    "$\\theta := \\theta - \\frac{\\alpha}{m}X^T(X\\theta-y)$\n",
    "\n",
    "入力が1特徴量（変数）の時は、バイアス項に対するパラメータを$\\theta_0$、入力変数に対するパラメータを$\\theta_1$としてパラメータは以下の様に更新されます。\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha  \\Sigma_{i=1}^m (h(x^{(i)})-y^{(i)})/m$ \n",
    "\n",
    "$\\theta_1 := \\theta_1 - \\alpha  \\Sigma_{i=1}^m ((h(x^{(i)})-y^{(i)})x^{(i)})/m$\n",
    "\n",
    "具体的に、`graddes`関数では以下の手順によりパラメータの学習を行います。\n",
    "\n",
    "- 引数`n_iter`で指定されたエポックの回数だけ以下を繰り返す    \n",
    "\n",
    "     - すべてのm個のデータについて以下を求める\n",
    "        \n",
    "        - 入力データ$x^{(i)}$について仮説関数$h(x^{(i)})$の値を求める\n",
    "        \n",
    "        - 出力$y^{(i)}$との誤差$h(x^{(i)})-y^{(i)}$の値を求める\n",
    "    \n",
    "     - すべてのm個のデータの誤差を用いてコスト関数$J(\\theta)$の値を求め、各エポックのコスト関数の値を要素とするリスト`costs`に追加\n",
    " \n",
    "     - すべてのm個のデータの誤差を用いて各パラメータ$\\theta_j(j=0,..,n-1)$を更新し、パラメータの値を要素とする配列`w`を更新\n",
    "          - `w[0,0]`$:=$($x_0$に対するパラメータ$\\theta_0$), \n",
    "          - ...,\n",
    "          - `w[n-1,0]`$:=$($x_{n-1}$に対するパラメータ$\\theta_{n-1}$)\n",
    "          \n",
    " すべての繰り返しが終了したらリスト`costs`と配列`w`を返す。\n",
    " \n",
    " 上記に従って、`graddes`関数を完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddes(X, y, alpha, n_iter):  \n",
    "    m = X.shape[0] # データ数\n",
    "    n =  X.shape[1] # 次元（特徴量+バイアス）数\n",
    "    \n",
    "    costs=[] # エポックごとのコスト関数の値を入れるリスト\n",
    "    w = np.zeros((n,1)) #  各特徴量に対するパラメータ（重み）の初期化\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "\n",
    "        ### コスト関数の計算とパラメータの更新をするコード ###\n",
    "            # graddes関数は任意の次元数の入力Xを想定していますが、この一般的な実装が難しければ\n",
    "            # まずは上記の定義に従って以下のように1変数+バイアス項の入力（対応するパラメータは2つ）で動くものを実装してもらっても構いません\n",
    "                # すべてのm個のデータの誤差(h(x)-y)を求める\n",
    "                # コスト関数Jを計算\n",
    "                #　costsにコスト関数の値を追加\n",
    "                # パラメータθ_0とθ_1を計算\n",
    "                # w[0,0]=更新されたパラメータθ_0,\n",
    "                # w[1,0]=更新されたパラメータθ_1\n",
    "            \n",
    "            # パラメータ更新ではデータ数についてのfor文とパラメータ数のfor文を回して更新していくことが考えられますが\n",
    "            # 上記のパラメータ全体を更新する行列・ベクトル演算でこれらのfor文なしにパラメータを更新できます。\n",
    "            # numpyに慣れている人はこちらの方針で実装してもらって構いません\n",
    "        \n",
    "    return costs, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4 最急降下法によるパラメータ推定\n",
    "`graddes`関数が完成したら以下のセルを実行して動作を確認してください。上記のirisデータセットの特徴量`petal_length`を入力、特徴量`petal_length`を出力とした訓練データセットを与え、学習率を0.01、学習のエポック数を100とした時の各エポックごとのコスト関数の値を示しています。パラメータの学習が進むにつれてコスト関数の値が減少していくことがわかります。\n",
    "\n",
    "この時の最終的なコスト関数の値は$\\simeq 0.036$、パラメータ$\\theta_0, \\theta_1$の値はそれぞれ約$\\simeq -3.6e^{-16}$, $\\simeq 9.5e^{-1}$となります。（評価はコードを見て行うので必ずしも一致していなくとも構いません）\n",
    "\n",
    "**学習率$\\alpha$を$0.01$, $0.1$と変更して、その時のコスト関数の曲線と100エポックでのコスト関数の値をそれぞれ観察してください。**それぞれ100エポックで十分にパラメータの推定がされたといえるでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=100\n",
    "alpha=0.05\n",
    "costs, w = graddes(X_norm, y_norm, alpha, n_iter)\n",
    "print(costs[-1]) # 100エポックでのコスト関数の値\n",
    "print(w) # 推定されたパラメータ\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iteration');\n",
    "plt.plot(range(1,n_iter+1),costs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100回のエポックで学習されたパラメータ$\\theta_0, \\theta_1$を用いて特徴量`petal_length`を入力$x$、特徴量`petal_length`を出力$y$とした時の直線$y=\\theta_0+\\theta_1x$は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 回帰直線のプロット\n",
    "def lineplot(X,y,w):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.xlabel('petal_length')\n",
    "    plt.ylabel('petal_width')\n",
    "    plt.scatter(X[:,1],y[:,0],c='blue')\n",
    "    plt.plot(X[:,1], np.dot(X,w)[:,0], color='red');\n",
    "\n",
    "# コスト関数のプロット\n",
    "def costplot(X, y, w, cost):\n",
    "    w0, w1 = np.meshgrid(np.arange(-3.0, 3.0, 0.1), np.arange(-3.0, 3.0, 0.1))\n",
    "    J=np.zeros(w0.shape)\n",
    "    for i in range(w0.shape[0]):\n",
    "        for j in range(w0.shape[1]):\n",
    "            J[i,j] = np.sum((np.dot(X, np.array([[w0[i,j]],[w1[i,j]]]))-y)**2)/(2*X.shape[0])\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_xlabel(\"theta0\")\n",
    "    ax.set_ylabel(\"theta1\")\n",
    "    ax.scatter(w[0,0], w[1,0], cost, s=100, c='red')\n",
    "    ax.plot_wireframe(w0, w1, J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=100\n",
    "alpha=0.05\n",
    "costs, w = graddes(X_norm, y_norm, alpha, n_iter)\n",
    "print(costs[-1])\n",
    "print(w)\n",
    "\n",
    "lineplot(X_norm, y_norm, w)\n",
    "costplot(X_norm, y_norm, w, costs[-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一方、5回、20回のエポックで学習されたパラメータを用いた時の直線はそれぞれ以下のようになります。エポックが進むにつれて、訓練データセットにフィッティングするようにパラメータが学習されていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=5\n",
    "alpha=0.05\n",
    "costs, w = graddes(X_norm, y_norm, alpha, n_iter)\n",
    "print(costs[-1])\n",
    "print(w)\n",
    "\n",
    "lineplot(X_norm, y_norm, w)\n",
    "costplot(X_norm, y_norm, w, costs[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=20\n",
    "alpha=0.05\n",
    "costs, w = graddes(X_norm, y_norm, alpha, n_iter)\n",
    "print(costs[-1])\n",
    "print(w)\n",
    "\n",
    "lineplot(X_norm, y_norm, w)\n",
    "costplot(X_norm, y_norm, w, costs[-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 正規方程式\n",
    "線形回帰のパラメータは訓練データセットの入力$X$と出力$y$に対して以下の正規方程式を解くことで解析的に求めることができます。（ただし、$X^TX$が正則（フルランク）であること。）\n",
    "\n",
    "$\\theta = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "ここで、行列$A$の転置$A^T$、逆行列$A^{-1}$は`NumPy`を用いてそれぞれ以下の様に計算できます。\n",
    "\n",
    "転置$A^T$\n",
    "```Python\n",
    "A.T\n",
    "```\n",
    "\n",
    "逆行列$A^{-1}$\n",
    "```Python\n",
    "np.linalg.inv(A)\n",
    "```\n",
    "\n",
    "正規方程式を用いて、訓練データセットを元に入力から出力を予測する仮説関数のパラメータを求める`normal_equation`関数を実装してください。`normal_equation`関数では第1引数に入力のデータ行列（データ数($m$)$\\times$次元数($n$))、第2引数に入力の各データに対する出力（正解）のベクトル（$m\\times1$）を受け取り、パラメータ$\\theta$の値を要素とする配列（$n\\times1$）（Q1.3のパラメータの配列`w`と同様の形式）を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    return ### 正規方程式を用いてX,yからパラメータを求めるコード ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normal_equation`関数が完成したら以下のセルを実行して動作を確認してください。先の最急降下法で求めたパラメータの推定値が正規方程式に基づくパラメータの解析解とよく近似していることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=normal_equation(X_norm, y_norm)\n",
    "print(w)\n",
    "\n",
    "lineplot(X_norm, y_norm, w)\n",
    "costplot(X_norm, y_norm, w, costs[-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 応用\n",
    "\n",
    "課題1と課題3で用いた以下のような形式の\"SSDSE.csv\"ファイルを読み込みます。\n",
    "\n",
    "```Python\n",
    "## SSDSE.csvファイル\n",
    "code, prefecture, municipality, A1101,\tA110101, A110102, ..., A4101\n",
    "地域コード, 都道府県, 市区町村, 人口総数, 人口総数（男）, 人口総数（女）, ..., 出生数\n",
    "R01100, 北海道, 札幌市, 1952356, 910614, 1041742, ..., 14021\n",
    "R01202, 北海道, 函館市, 265979, 120376, 145603, ..., 1532\n",
    "R01203, 北海道, 小樽市, 121924, 54985, 66939, ..., 512\n",
    "...\n",
    "```\n",
    "\n",
    "`pandas`モジュールでは、\"SSDES.csv\"ファイルから以下のようにデータフレーム`df`を作成できます。この時、csvファイルの2行目（日本語ヘッダ）は`skiprows=[1]`で読み飛ばしています。\n",
    "```Python\n",
    "df = pd.read_csv('SSDSE.csv',  skiprows=[1])\n",
    "```\n",
    "\n",
    "東京都の市区町村の人口総数（A1101）と出生数（A4101）の関係を散布図で可視化すると以下のように2つの統計に関係があることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SSDSE.csv', skiprows=[1])\n",
    "df=df[df['prefecture']=='東京都']\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('# of births')\n",
    "plt.scatter(df['A1101'], df['A4101'], c='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のQ1で行った線形回帰のパラメータ推定を参考に、人口総数$x$から出生数$y$を予測するような仮説関数$y=\\theta_0+\\theta_1x$のパラメータ$\\theta_0, \\theta_1$を学習し、学習されたパラメータを元に回帰直線を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['A1101']].values # 入力：人口総数\n",
    "y=df[['A4101']].values # 出力: 出生数\n",
    "\n",
    "X_norm = normalizer(X) # 入力の標準化\n",
    "y_norm = normalizer(y) # 出力の標準化\n",
    "\n",
    "X_norm=np.insert(X_norm, 0, np.ones((1,X.shape[0]), dtype=int), axis=1) # バイアス項の追加\n",
    "\n",
    "###  線形回帰により人口総数から出生数を予測するような仮説関数のパラメータを学習し、学習されたパラメータを元に回帰直線を可視化 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考：scikit-learnでの線形回帰\n",
    "`scikit-learn`では、以下の手順でデータからモデルの学習を行います。\n",
    "- 使用するモデルのクラスの選択\n",
    "- モデルのハイパーパラメータの選択とインスタンス化\n",
    "- データの準備\n",
    "    - 教師あり学習では、特徴量データとラベルデータを準備\n",
    "    - 教師あり学習では、特徴量・ラベルデータをモデル学習用の学習データとモデル評価用のテストデータに分ける\n",
    "- モデルをデータに適合（`fit()`メソッド）\n",
    "- モデルの評価\n",
    "    - 教師あり学習では、`predict()`メソッドを用いてテストデータの特徴量データからラベルデータを予測しその精度を評価を行う\n",
    "    \n",
    "以下では、回帰を行うモデルの一つである**線形回帰**（**`LinearRegression`**）クラスをインポートしています。`mean_squared_error()`は平均二乗誤差によりモデルの予測精度を評価するための関数です。\n",
    "\n",
    "データセットを訓練データ（`X_train`, `y_train`）とテストデータ（`X_test`, `y_test`)に分割し、線形回帰クラスのインスタンスの`fit()`メソッドによりモデルを訓練データに適合させています。そして、`predict()`メソッドを用いてテストデータのpetal_lengthの値からpetal_widthの値を予測し、`mean_squared_error()`関数で実際のpetal_widthの値（`y_test`）と比較して予測精度の評価を行なっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "iris = load_iris() \n",
    "X=(iris['data'][:,2])[:,np.newaxis] \n",
    "y=(iris['data'][:,3])[:,np.newaxis] \n",
    "\n",
    "# 訓練データとテストデータ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "sc=StandardScaler() \n",
    "sc.fit(X_train)\n",
    "norm_X_train=sc.transform(X_train) \n",
    "norm_X_test=sc.transform(X_test) \n",
    "\n",
    "sc.fit(y_train)\n",
    "norm_y_train=sc.transform(y_train) \n",
    "norm_y_test=sc.transform(y_test) \n",
    "\n",
    "# 自分でバイアス項追加する場合 \n",
    "# LinearRegression()ではデフォルトでバイアス項が作られる（fit_intercept引数=True）\n",
    "#norm_X_train=np.insert(norm_X_train, 0, np.ones((norm_X_train.shape[0],1), dtype=int).T, axis =1)\n",
    "#norm_X_test=np.insert(norm_X_test, 0, np.ones((norm_X_test.shape[0],1), dtype=int).T, axis =1)\n",
    "\n",
    "# 学習モデルの訓練データへの適合\n",
    "model=LinearRegression() \n",
    "model.fit(norm_X_train, norm_y_train) \n",
    "\n",
    "# 学習モデルを用いてテストデータから予測と評価\n",
    "y_predicted=model.predict(norm_X_test) \n",
    "print(mean_squared_error(norm_y_test,y_predicted))\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel(iris[\"feature_names\"][2])\n",
    "plt.ylabel(iris[\"feature_names\"][3])\n",
    "plt.scatter(norm_X_train[:,0], norm_y_train[:,0],c='blue') # 訓練データ\n",
    "plt.scatter(norm_X_test[:,0], y_predicted[:,0], c='red'); # テストデータ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
