{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題9 \n",
    "配点\n",
    "- Q1.1 1P\n",
    "- Q1.2 2P\n",
    "- Q2.1 2P\n",
    "- Q2.2 2P\n",
    "- Q3 3P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot  as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 多項式による線形回帰\n",
    "`scikit-learn`モジュールの`PolynomialFeatures`クラスを使うと入力の特徴量から多項や交差項の特徴量を生成することができます。多項や交差項の特徴量を導入にすることによりモデルが複雑になり、訓練データへのより柔軟な適合が可能になります。\n",
    "\n",
    "例えば、多項式の次数としてパラメータ`degree`に2を指定すると、1変数の入力$x$に対して、1（バイアス項), $x$, $x^2$、の3つの特徴量を生成します。2変数入力$x_1, x_2$の場合は、1, $x_1$, $x_2$, $x_1^2$, $x_1x_2$, $x_2^2$、の特徴量が生成されます。バイアス項を生成しない場合はパラメータ`include_bias`に`False`を指定します。\n",
    "\n",
    "[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "iris = load_iris() \n",
    "X=(iris['data'][:,2])[:,np.newaxis] \n",
    "y=(iris['data'][:,3])[:,np.newaxis] \n",
    "\n",
    "print(PolynomialFeatures(degree=2).fit_transform(X)) # 1, x, x^2、の特徴量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の関数`iris_poly_regression()`では、課題8で用いたirisデータセットの特徴量`petal_length`から引数`n`の次数の多項式の特徴量を生成し、それらの特徴量を用いて線形回帰（`LinearRegression`）により特徴量`petal_width`を予測するモデルを学習しています。\n",
    "\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "データセットは`tran_test_split`関数を使って、70%をモデル学習の訓練データ、30%をモデル評価のテストデータに分割しています。\n",
    "\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "モデル評価は平均二乗誤差（`mean_squared_error`）で行います。\n",
    "\n",
    "[mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def iris_poly_regression(n):\n",
    "    # 引数 n: 多項式の次数 \n",
    "\n",
    "    iris = load_iris() \n",
    "    X=(iris['data'][:,2])[:,np.newaxis] \n",
    "    y=(iris['data'][:,3])[:,np.newaxis] \n",
    "\n",
    "    # 訓練データとテストデータに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # 入力からn次多項式の特徴量を生成\n",
    "    poly=PolynomialFeatures(degree=n, include_bias=False)\n",
    "    poly_X_train=poly.fit_transform(X_train)\n",
    "    poly_X_test=poly.fit_transform(X_test)\n",
    "\n",
    "    # 訓練データへ適合\n",
    "    model=LinearRegression(normalize=True) # normalize=Trueで各特徴量を標準化\n",
    "    model.fit(poly_X_train, y_train) \n",
    "\n",
    "    # 訓練データ誤差\n",
    "    y_predicted_train=model.predict(poly_X_train)\n",
    "    train_loss =mean_squared_error(y_train,y_predicted_train)\n",
    "    print(train_loss)\n",
    "\n",
    "    # テストデータ誤差\n",
    "    y_predicted_test=model.predict(poly_X_test) \n",
    "    test_loss = mean_squared_error(y_test,y_predicted_test)\n",
    "    print(test_loss)\n",
    "\n",
    "    #パラメータ\n",
    "    #print(model.intercept_)\n",
    "    #print(model.coef_)\n",
    "\n",
    "    # 曲線描画用のデータポイント\n",
    "    X_point=np.arange(X.min(),X.max(),0.1)[:,np.newaxis]\n",
    "    y_point=model.predict(poly.fit_transform(X_point))\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.xlabel(iris[\"feature_names\"][2])\n",
    "    plt.ylabel(iris[\"feature_names\"][3])\n",
    "    plt.scatter(poly_X_train[:,0], y_train,c='blue') # 訓練データ\n",
    "    plt.scatter(poly_X_test[:,0], y_test,c='green') # テストデータ\n",
    "    plt.plot(X_point, y_point, color='red');\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多項式の次数$n$を変化させて、入力（petal_lengthの値）と出力（入力から予測されたpetal_widthの値）の対応がどのように変化するか、特に次数が大きくなり学習モデルが複雑になると訓練データ（青のデータポイント）に適合しすぎてしまう過学習が起こることを観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_regression(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_regression(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多項式の次数を1から10まで変化させた時の訓練データとテストデータの誤差の変化をプロットする以下の処理を完成させてください。\n",
    "\n",
    "完成したらセルを実行し、多項式の次数が大きくなると、訓練データとテストデータの誤差はそれぞれどのように変化するか観察してください。特に、次数が大きくなると訓練データの誤差が減少していきますが、テストデータの誤差は必ずしも減少しないことも確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=np.arange(1,11) # 次数(1から10)\n",
    "train_curve=[] # 各次数の訓練データ誤差のリスト\n",
    "test_curve=[] # 各次数のテストデータ誤差のリスト\n",
    "\n",
    "iris = load_iris() \n",
    "X=(iris['data'][:,2])[:,np.newaxis] \n",
    "y=(iris['data'][:,3])[:,np.newaxis] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "for n in degree:\n",
    "    # 入力からn次多項式の特徴量を生成\n",
    "    poly=PolynomialFeatures(degree=n, include_bias=False)\n",
    "    poly_X_train=poly.fit_transform(X_train)\n",
    "    poly_X_test=poly.fit_transform(X_test)\n",
    "\n",
    "    # 訓練データへ適合\n",
    "    model=LinearRegression(normalize=True) # normalize=Trueで各特徴量を標準化\n",
    "    model.fit(poly_X_train, y_train) \n",
    "\n",
    "    # 訓練データ誤差\n",
    "    y_predicted_train=model.predict(poly_X_train)\n",
    "    train_loss = ### 訓練データの誤差を計算するコード ###\n",
    "    ### train_curveにtrain_lossを追加するコード ###\n",
    "\n",
    "    # テストデータ誤差\n",
    "    y_predicted_test=model.predict(poly_X_test) \n",
    "    test_loss = ### テストデータの誤差を計算するコード ###\n",
    "    ### test_curveにtest_lossを追加するコード ###\n",
    "    \n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('mean squared error')    \n",
    "plt.plot(degree, train_curve, color='red'); # 訓練データ誤差曲線\n",
    "plt.plot(degree, test_curve, color='blue'); # テストデータ誤差曲線"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 正則化\n",
    "以下では、学習モデルとして`LinearRegression`の代わりに`Ridge`（リッジ回帰）を使っています。`Ridge`クラスは、$L_2$ノルムを正則化に用いた線形回帰モデルで学習を行うことができます。パラメータ`alpha`に正則化項の係数（講義資料中の$\\lambda$）を指定します。\n",
    "\n",
    "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "Q1.1で観察した過学習が、正則化により抑えられていることを観察してください。また、正則化項の係数のハイパーパラメータを変化させた時（例えば、0.01, 0.1, 1, 10など）、入力（`petal_length`の値）と出力（入力から予測された`petal_width`の値）の対応がどのように変化するかその時の誤差の値とともに観察してください。正則化項の係数を大きするとアンダーフィッティングが起こることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def iris_poly_ridge(n, a):\n",
    "    # 引数 n: 多項式の次数 \n",
    "    # 引数 a: 正則化項の係数\n",
    "\n",
    "    iris = load_iris() \n",
    "    X=(iris['data'][:,2])[:,np.newaxis] \n",
    "    y=(iris['data'][:,3])[:,np.newaxis] \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # 入力からn次多項式の特徴量を生成\n",
    "    poly=PolynomialFeatures(degree=n, include_bias=False)\n",
    "    poly_X_train=poly.fit_transform(X_train)\n",
    "    poly_X_test=poly.fit_transform(X_test)\n",
    "\n",
    "    # 訓練データへ適合\n",
    "    # リッジ回帰モデル（aは正則化項の係数）\n",
    "    model=Ridge(alpha=a, normalize=True)\n",
    "    model.fit(poly_X_train, y_train) \n",
    "\n",
    "    # 訓練データ誤差\n",
    "    y_predicted_train=model.predict(poly_X_train)\n",
    "    train_loss =mean_squared_error(y_train,y_predicted_train)\n",
    "    print(train_loss)\n",
    "\n",
    "    # テストデータ誤差\n",
    "    y_predicted_test=model.predict(poly_X_test) \n",
    "    test_loss = mean_squared_error(y_test,y_predicted_test)\n",
    "    print(test_loss)\n",
    "\n",
    "    #パラメータ\n",
    "    #print(model.intercept_)\n",
    "    #print(model.coef_)\n",
    "\n",
    "    # 曲線描画用のデータポイント\n",
    "    X_point=np.arange(X.min(),X.max(),0.1)[:,np.newaxis]\n",
    "    y_point=model.predict(poly.fit_transform(X_point))\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.xlabel(iris[\"feature_names\"][2])\n",
    "    plt.ylabel(iris[\"feature_names\"][3])\n",
    "    plt.scatter(poly_X_train[:,0], y_train,c='blue') # 訓練データ\n",
    "    plt.scatter(poly_X_test[:,0], y_test,c='green') # テストデータ\n",
    "    plt.plot(X_point, y_point, color='red');\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_ridge(1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_ridge(3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = iris_poly_ridge(10, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.1を参考に、上記のリッジ回帰で多項式の次数を1から10まで変化させた時の訓練データとテストデータの誤差の変化をプロットする以下の処理を完成させてください。その際、正則化項の係数は0.01など適当な値に固定してください。\n",
    "\n",
    "完成したらセルを実行し、多項式の次数が大きくなると、訓練データとテストデータの誤差はそれぞれどのように変化するか観察してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 上記のリッジ回帰で多項式の次数を1から10まで変化させた時の訓練データとテストデータの誤差の変化曲線を表示するコード ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irisデータセットの特徴量`petal_length`と`petal_width`で2つの花の種類`versicolor`か`virginica`のデータを散布図で可視化すると以下のように花ごとに`petal_length`と`petal_width`の特徴が異なることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X_all = iris.data[:,[2,3]] # 特徴量petal_lengthとpetal_width\n",
    "y_all = iris.target # 花の種類\n",
    "\n",
    "X = X_all[(y_all==1)|(y_all==2)] # 花の種類versicolorかvirginicaに対応する入力\n",
    "y_label = y_all[(y_all==1)|(y_all==2)]-1 # 花の種類versicolorかvirginica（ラベルを0or1にする）\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.scatter(X[:,0], X[:,1], c=y_label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、ロジスティック回帰により特徴量`petal_length`と`petal_width`から2つの花の種類`versicolor`か`virginica`を予測するような仮説関数のパラメータを学習することを考えます。\n",
    "\n",
    "まず準備として、特徴量`petal_length`と`petal_width`を入力$X$, 花の種類`versicolor`か`virginica`を出力（ラベル）$y$とします。入力$X$は各特徴量ごとに標準化し、入力の各データにバイアス項($x_0=1$)を追加するため、入力の先頭列に1を要素とする列ベクトルを挿入します。これにより、入力$X$と出力$y$はデータ数を$m$としてそれぞれ行列（$m \\times 3$) とベクトル($m \\times 1$)になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_label[:,np.newaxis] # 列ベクトルにする\n",
    "X_norm=(X-np.mean(X, axis=0))/np.std(X, axis=0) # 標準化\n",
    "X_norm=np.insert(X_norm, 0, np.ones((1,X.shape[0]), dtype=int), axis=1) # バイアス項の追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 シグモイド関数\n",
    "シグモイド関数$g(z)$は以下のように定義されます\n",
    "\n",
    "$g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "引数に`NumPy`の実数値を要素とする任意の長さの配列を入力として受け取り、配列の各要素に対するシグモイド関数の値を要素とする配列を返す`sigmoid`関数を完成させてください。\n",
    "\n",
    "[NumPyの指数関数](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return ### 配列Zの各要素に対するシグモイド関数の値を計算しそれら要素とする配列を返すコード ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sigmoid`関数は入力の配列`[-1,0,1]`に対しては、配列`[0.26894142, 0.5, 0.73105858]`を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.array([-1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 コスト関数とパラメータの推定\n",
    "\n",
    "以下では、最急降下法により、訓練データセットを元にロジスティック回帰の仮説関数のパラメータを学習する`graddes_logit`関数を実装します。\n",
    "\n",
    "`graddes_logit`関数では第1引数に入力のデータ行列（データ数($m$)$\\times$(特徴量+バイアス)($n$))、第2引数に入力の各データに対する出力（正解）のベクトル（$m\\times1$）、第3引数に学習率、第4引数に学習の繰り返し（各繰り返しをエポックと呼ぶ）の回数を受け取ります。\n",
    "\n",
    "これらの引数を元に、`graddes_logit`関数ではパラメータの学習を行い、以下を返します\n",
    "\n",
    "- エポックごとのコスト関数の値を要素とするリスト\n",
    "- 最終的なパラメータの値を要素とする配列（$n\\times1$）を返します。\n",
    "\n",
    "\n",
    "パラメータを$\\theta=(\\theta_0, \\theta_1, ..., \\theta_{n-1})^T$\n",
    "\n",
    "仮説関数を$h(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2+....+\\theta_{n-1}x_{n-1})=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "入力を$\n",
    "  X = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      x_0^{(1)} &   x_1^{(1)} & ... &   x_{n-1}^{(1)}  \\\\\n",
    "      ... & ...& ...&...\\\\\n",
    "      x_0^{(m)} &  x_1^{(m)} & ... &   x_{n-1}^{(m)}  \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$ \n",
    "\n",
    "$X$において$x_0^{(i)}=1$\n",
    "\n",
    "出力を$y=(y^{(1)}, y^{(2)}, ..., y^{(m)})^T$\n",
    "\n",
    "とすると、ロジスティック回帰のコスト関数を以下のようにして\n",
    "\n",
    "$J(\\theta)=-\\frac{1}{m}\\Sigma_{i=1}^{m}(y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)})))$\n",
    "\n",
    "最急降下法では入力$X$の各特徴量$x_j$に対するパラメータ$\\theta_j$を以下の様に更新していきます。\n",
    "\n",
    "$\\theta_j:= \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} =  \\theta_j - \\frac{\\alpha}{m} \\Sigma_{i=1}^m ((h(x^{(i)})-y^{(i)})x^{(i)}_j)$\n",
    "\n",
    "パラメータ全体を以下のように一度に更新することもできます。\n",
    "\n",
    "$\\theta := \\theta - \\frac{\\alpha}{m}X^T(g(X\\theta)-y)$\n",
    "\n",
    "入力が1特徴量（変数）の時は、バイアス項に対するパラメータを$\\theta_0$、入力変数に対するパラメータを$\\theta_1$としてパラメータは以下の様に更新されます。\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\frac{\\alpha}{m}\\Sigma_{i=1}^m (h(x^{(i)})-y^{(i)})$ \n",
    "\n",
    "$\\theta_1 := \\theta_1 - \\frac{\\alpha}{m}\\Sigma_{i=1}^m ((h(x^{(i)})-y^{(i)})x^{(i)})$\n",
    "\n",
    "具体的に、`graddes_logit`関数では以下の手順によりパラメータの学習を行います。\n",
    "\n",
    "- 引数`n_iter`で指定されたエポックの回数だけ以下を繰り返す    \n",
    "\n",
    "     - すべてのm個のデータについて以下を求める\n",
    "        \n",
    "        - 入力データ$x^{(i)}$について仮説関数$h(x^{(i)})$の値を求める\n",
    "        \n",
    "        - 出力$y^{(i)}$との誤差$h(x^{(i)})-y^{(i)}$の値を求める\n",
    "    \n",
    " - すべてのm個のデータの誤差を用いてコスト関数$ J(\\theta)$の値を求め、各エポックのコスト関数の値を要素とするリスト`costs`に追加\n",
    " \n",
    " - すべてのm個のデータの誤差を用いて各パラメータ$\\theta_j (j=0,..,n-1)$を更新し、パラメータの値を要素とする配列`w`を更新\n",
    "      - `w[0,0]`$:=$($x_0$に対するパラメータ$\\theta_0$), \n",
    "      - ...,\n",
    "      - `w[n-1,0]`$:=$($x_{n-1}$に対するパラメータ$\\theta_{n-1}$)\n",
    "        \n",
    "\n",
    " \n",
    " すべての繰り返しが終了したらリスト`costs`と配列`w`を返す。\n",
    " \n",
    " 上記に従って、`graddes_logit`関数を完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddes_logit(X, y, alpha, n_iter):  \n",
    "    m = X.shape[0] # データ数\n",
    "    n =  X.shape[1] # 次元（特徴量）数\n",
    "    \n",
    "    costs=[] # エポックごとのコスト関数の値を入れるリスト\n",
    "    w = np.zeros((n,1)) #  バイアスと各特徴量に対するパラメータ（重み）の初期化\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "\n",
    "        ### コスト関数の計算とパラメータの更新をするコード ###\n",
    "            # 課題8のgraddes関数と手続きは同じです\n",
    "            # ロジスティック回帰では仮説関数h(x)がシグモイド関数になっていることに注意してください\n",
    "            # ロジスティック回帰のコスト関数にすることに注意してください\n",
    "        \n",
    "    return costs, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`graddes_logit`関数が完成したら以下のセルを実行して動作を確認してください。`graddes_logit`関数に訓練データセットを与え、学習率を0.1、学習のエポック数を100とした時の各エポックごとのコスト関数の値を示しています。パラメータの学習が進むにつれてコスト関数の値が減少していくことがわかります。この時の最終的なコスト関数の値は$\\simeq 0.199$となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.1 # 学習率\n",
    "n=100 # 繰り返し回数\n",
    "costs, w = graddes_logit(X_norm, y, a, n)\n",
    "print(costs[-1])\n",
    "print(w)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,n+1),costs) # 繰り返しとコスト関数のプロット\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習されたパラメータを元に、2つの特徴量、`petal_length`と`petal_width`、から2つの花の種類、`versicolor`か`virginica`、を分類するための決定境界を可視化してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "x1=np.arange(X_norm[:,1].min(),X_norm[:,1].max(),0.1)\n",
    "plt.plot(x1, -(w[1,0]*x1+w[0,0])/w[2,0])\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.scatter(X_norm[:,1], X_norm[:,2],c=y_label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの特徴量、`petal_length`($x_1$)と`petal_width`($x_2$)に対する、仮説関数$y=\\frac{1}{1+e^{-(\\theta_0+\\theta_1x_1+\\theta_2x_2)}}$（ロジスティック関数）は以下のようになります。以下の曲面で$y=0.5$となるところが上記の決定境界になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X1, X2 = np.meshgrid(np.arange(-2.0, 2.0, 0.1), np.arange(-2.0, 2.0, 0.1))\n",
    "Z = sigmoid(w[0,0]+w[1,0]*X1+ w[2,0]*X2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.set_xlabel(\"petal_length\")\n",
    "ax.set_ylabel(\"petal_widht\")\n",
    "ax.plot_wireframe(X1, X2, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "以下のような形式の\"SSDSE.csv\"ファイルを読み込みます。\n",
    "\n",
    "```Python\n",
    "## SSDSE.csvファイル\n",
    "code, prefecture, municipality, A1101,\tA110101, A110102, ..., A4101\n",
    "地域コード, 都道府県, 市区町村, 人口総数, 人口総数（男）, 人口総数（女）, ..., 出生数\n",
    "R01100, 北海道, 札幌市, 1952356, 910614, 1041742, ..., 14021\n",
    "R01202, 北海道, 函館市, 265979, 120376, 145603, ..., 1532\n",
    "R01203, 北海道, 小樽市, 121924, 54985, 66939, ..., 512\n",
    "...\n",
    "```\n",
    "\n",
    "`pandas`モジュールでは、\"SSDES.csv\"ファイルから以下のようにデータフレーム`df`を作成できます。この時、csvファイルの2行目（日本語ヘッダ）は`skiprows=[1]`で読み飛ばしています。\n",
    "```Python\n",
    "df = pd.read_csv('SSDSE.csv',  skiprows=[1])\n",
    "```\n",
    "\n",
    "北海道と東京都の各市区町村（札幌市を除く）を人口総数（A1101）と面積（B1101）で散布図により可視化すると以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SSDSE.csv', skiprows=[1])\n",
    "df=df[((df['prefecture']=='北海道') & (df['municipality'] !='札幌市'))|(df['prefecture']=='東京都')]\n",
    "df.loc[df['prefecture'] == '北海道', 'prefecture'] = 1 # 北海道の市区町村ラベルを1\n",
    "df.loc[df['prefecture'] == '東京都', 'prefecture'] = 0 # 東京都の市区町村ラベルを0\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('land area')\n",
    "plt.scatter(df['A1101'], df['B1101'],c=df['prefecture']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のQ2で行ったロジスティック回帰のパラメータ推定を参考に、市区長村の人口総数$x_1$と面積$x_2$からその市区町村が北海道（1）か東京都（0）かを予測するような仮説関数$y=\\frac{1}{1+e^{-(\\theta_0+\\theta_1x_1+\\theta_2x_2)}}$のパラメータ$\\theta_0, \\theta_1, \\theta_2$を学習し、学習されたパラメータを元に決定境界を可視化してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['A1101','B1101']].values # 入力: 人口総数と面積\n",
    "y=df[['prefecture']].values # 出力: 北海道の市区町村（1）か東京都の市区町村（0）\n",
    "\n",
    "X_norm=(X-np.mean(X, axis=0))/np.std(X, axis=0) # 標準化\n",
    "X_norm=np.insert(X_norm, 0, np.ones((1,X.shape[0]), dtype=int), axis=1) # バイアス項の追加\n",
    "\n",
    "###  ロジスティック回帰により、市区長村の人口総数と面積からその市区町村が北海道（1）か東京都（0）かを予測するような\n",
    "###  仮説関数のパラメータを学習し、学習されたパラメータを元に決定境界を可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考：scikit-learnでのロジスティック回帰\n",
    "以下では、ロジスティック回帰（`LogisticRegression`）クラスをインポートしています。`train_test_split()`はデータセットを学習データとテストデータに分割するための関数、`accuracy_score()`はモデルの予測精度を評価するための関数です。\n",
    "\n",
    "[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "特徴量データ（`X_irist`）とラベルデータ（`y_iris`）からなるデータセットを学習データ（`X_train`, `y_train`）とテストデータ（`X_test`, `y_test`)に分割しています。ここでは、`train_test_split()`関数の`test_size`引数にデータセットの30%をテストデータとすることを指定しています。また、`stratify`引数にラベルデータを指定することで、学習データとテストデータ、それぞれでラベルの分布が同じになるようにしています。入力の特徴量は`StandardScaler`で標準化しています。\n",
    "\n",
    "ロジスティック回帰クラスのインスタンスを作成し、**`fit`**`()`メソッドによりモデルを学習データに適合させています。そして、**`predict`**`()`メソッドを用いてテストデータのラベルを予測し、`accuracy_score()`関数などで実際のラベルデータ（`y_test`）と比較して予測精度の評価を行なっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0]\n",
      " [ 1 14]]\n",
      "0.9666666666666667\n",
      "0.9375\n",
      "1.0\n",
      "0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_all = iris.data[:,[2,3]] \n",
    "y_all = iris.target \n",
    "X = X_all[(y_all==1)|(y_all==2)] # 入力\n",
    "y = y_all[(y_all==1)|(y_all==2)] # 出力\n",
    "\n",
    "# 学習データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 標準化\n",
    "sc=StandardScaler() \n",
    "sc.fit(X_train)\n",
    "norm_X_train=sc.transform(X_train) \n",
    "norm_X_test=sc.transform(X_test) \n",
    "\n",
    "model=LogisticRegression(solver='lbfgs',  multi_class='auto') # ロジスティック回帰モデル\n",
    "# model=LogisticRegression() # sklearn0.19以前の場合\n",
    "model.fit(norm_X_train, y_train) # モデルを学習データに適合\n",
    "\n",
    "#print(model.coef_) # パラメータ重み\n",
    "#print(model.intercept_) # バイアス重み\n",
    "\n",
    "y_pred=model.predict(norm_X_test) # テストデータでラベルを予測\n",
    "\n",
    "# print(model.predict_proba(norm_X_test)) # 各テストデータのラベル1or0であるかの確率の推定値\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) # 混同行列\n",
    "print(accuracy_score(y_test, y_pred)) # accuracy\n",
    "print(precision_score(y_test, y_pred)) # precision\n",
    "print(recall_score(y_test, y_pred)) # recall\n",
    "print(f1_score(y_test, y_pred))  # f値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression`ではデフォルトで正則化（$L_2$ノルム）が有効になっています。引数`C`は正則化項の係数の逆数になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0]\n",
      " [ 1 14]]\n",
      "0.9666666666666667\n",
      "0.9375\n",
      "1.0\n",
      "0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_all = iris.data[:,[2,3]] \n",
    "y_all = iris.target \n",
    "X = X_all[(y_all==1)|(y_all==2)] # 入力\n",
    "y = y_all[(y_all==1)|(y_all==2)] # 出力\n",
    "\n",
    "# 学習データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 標準化\n",
    "sc=StandardScaler() \n",
    "sc.fit(X_train)\n",
    "norm_X_train=sc.transform(X_train) \n",
    "norm_X_test=sc.transform(X_test) \n",
    "\n",
    "model=LogisticRegression(solver='lbfgs',  multi_class='auto', C=100) # ロジスティック回帰モデル、正則化の係数を指定\n",
    "# model=LogisticRegression() # sklearn0.19以前の場合\n",
    "model.fit(norm_X_train, y_train) # モデルを学習データに適合\n",
    "\n",
    "#print(model.coef_) # パラメータ重み\n",
    "#print(model.intercept_) # バイアス重み\n",
    "\n",
    "y_pred=model.predict(norm_X_test) # テストデータでラベルを予測\n",
    "\n",
    "# print(model.predict_proba(norm_X_test)) # 各テストデータのラベル1or0であるかの確率の推定値\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) # 混同行列\n",
    "print(accuracy_score(y_test, y_pred)) # accuracy\n",
    "print(precision_score(y_test, y_pred)) # precision\n",
    "print(recall_score(y_test, y_pred)) # recall\n",
    "print(f1_score(y_test, y_pred))  # f値"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
